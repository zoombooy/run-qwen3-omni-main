import { EventEmitter } from 'eventemitter3'
import type {
  AudioConfig,
  AudioStream,
  AudioBuffer,
  AudioVisualizationData,
  AudioRecordingState,
  AudioPlaybackState,
  AudioDevice,
  AudioConstraints
} from '@/types/audio'
import { VadDetector } from '@/modules/vad/VadDetector'

export class AudioManager extends EventEmitter {
  private audioContext: AudioContext | null = null
  private mediaStream: MediaStream | null = null
  private sourceNode: MediaStreamAudioSourceNode | null = null
  private analyserNode: AnalyserNode | null = null
  private gainNode: GainNode | null = null
  private monitorGainNode: GainNode | null = null
  private audioElement: HTMLAudioElement | null = null
  private currentPlaybackUrl: string | null = null
  private playbackGainNode: GainNode | null = null
  private streamingSources: AudioBufferSourceNode[] = []
  private streamingState: { isActive: boolean; nextStartTime: number; hasChunks: boolean } = {
    isActive: false,
    nextStartTime: 0,
    hasChunks: false
  }
  private streamingStopTimeout: number | null = null
  private config: AudioConfig
  private recordingState: AudioRecordingState = {
    isRecording: false,
    isPaused: false,
    duration: 0,
    fileSize: 0,
    audioBuffer: [],
    currentBuffer: null
  }
  private connectionReady: boolean = false
  private playbackState: AudioPlaybackState = {
    isPlaying: false,
    isPaused: false,
    currentTime: 0,
    duration: 0,
    volume: 1.0,
    playbackRate: 1.0
  }
  private recordingTimer: number | null = null
  private animationFrame: number | null = null
  private devices: AudioDevice[] = []
  private vadDetector: VadDetector | null = null
  private lastVolume: number = 0  // ç”¨äºéŸ³é‡å¹³æ»‘å¤„ç†

  constructor(config: Partial<AudioConfig> = {}) {
    super()

    this.config = {
      sampleRate: 16000,
      channelCount: 1,
      bufferSize: 4096,
      inputVolume: 1.0,
      outputVolume: 1.0,
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true,
      ...config
    }
  }

  async initialize(): Promise<void> {
    try {
      // åˆ›å»ºAudioContext
      this.audioContext = new AudioContext({
        sampleRate: this.config.sampleRate
      })

      // åˆ›å»ºæ’­æ”¾å¢ç›ŠèŠ‚ç‚¹ï¼Œæ§åˆ¶TTSè¾“å‡ºéŸ³é‡
      this.playbackGainNode = this.audioContext.createGain()
      this.playbackGainNode.gain.value = this.config.outputVolume
      this.playbackGainNode.connect(this.audioContext.destination)

      // åˆ›å»ºéŸ³é¢‘å…ƒç´ 
      this.audioElement = new Audio()
      this.audioElement.volume = this.config.outputVolume

      // åˆå§‹åŒ–VADæ£€æµ‹å™¨
      this.vadDetector = new VadDetector({
        threshold: 5,
        silenceDuration: 1500, // é»˜è®¤1500æ¯«ç§’
        voiceStartCallback: () => {
          this.emit('voiceStarted');
        },
        voiceStopCallback: () => {
          this.emit('voiceStopped');
        }
      });

      // è·å–è®¾å¤‡åˆ—è¡¨
      await this.updateDeviceList()

      this.emit('initialized')
    } catch (error) {
      console.error('Audio initialization failed:', error)
      throw error
    }
  }

  async checkMicrophonePermission(): Promise<PermissionState> {
    try {
      const permission = await navigator.permissions.query({ name: 'microphone' as PermissionName })
      return permission.state
    } catch (error) {
      console.warn('Permission API not available, trying getUserMedia to check permission')
      return 'prompt'
    }
  }

  async requestMicrophonePermission(constraints?: AudioConstraints): Promise<boolean> {
    try {
      const mediaConstraints: MediaStreamConstraints = {
        audio: {
          sampleRate: { ideal: this.config.sampleRate },
          channelCount: { ideal: this.config.channelCount },
          echoCancellation: this.config.echoCancellation,
          noiseSuppression: this.config.noiseSuppression,
          autoGainControl: this.config.autoGainControl,
          deviceId: constraints?.deviceId
        }
      }

      const testStream = await navigator.mediaDevices.getUserMedia(mediaConstraints)
      testStream.getTracks().forEach(track => track.stop())
      return true
    } catch (error) {
      console.error('Microphone permission denied:', error)
      return false
    }
  }

  async startRecording(constraints?: AudioConstraints): Promise<void> {
    if (this.recordingState.isRecording) {
      return
    }

    try {
      // æ£€æŸ¥æƒé™
      const permission = await this.checkMicrophonePermission()
      if (permission === 'denied') {
        throw new Error('Microphone permission denied')
      }

      // è¯·æ±‚æƒé™
      const hasPermission = await this.requestMicrophonePermission(constraints)
      if (!hasPermission) {
        throw new Error('Failed to get microphone permission')
      }

      // è·å–åª’ä½“æµ
      const mediaConstraints: MediaStreamConstraints = {
        audio: {
          sampleRate: this.config.sampleRate,
          channelCount: this.config.channelCount,
          echoCancellation: this.config.echoCancellation,
          noiseSuppression: this.config.noiseSuppression,
          autoGainControl: this.config.autoGainControl,
          deviceId: constraints?.deviceId
        }
      }

      this.mediaStream = await navigator.mediaDevices.getUserMedia(mediaConstraints)

      if (this.audioContext?.state === 'suspended') {
        await this.audioContext.resume()
      }

      // æƒé™è·å–æˆåŠŸåï¼Œæ›´æ–°è®¾å¤‡åˆ—è¡¨
      await this.updateDeviceList()

      // æ·»åŠ æµè®¾ç½®
      if (this.mediaStream.getAudioTracks().length > 0) {
        const settings = this.mediaStream.getAudioTracks()[0].getSettings()
      }

      // æ£€æŸ¥éŸ³é¢‘è½¨é“
      const audioTracks = this.mediaStream.getAudioTracks()

      // åˆ›å»ºéŸ³é¢‘å¤„ç†èŠ‚ç‚¹
      this.setupAudioProcessing()
      console.log('ğŸ¤ Audio processing setup completed')

      // æµ‹è¯•éº¦å…‹é£è¾“å…¥
      this.testMicrophoneInput()

      // å¼€å§‹å½•åˆ¶
      this.recordingState.isRecording = true
      this.recordingState.isPaused = false
      this.recordingState.duration = 0
      this.recordingState.fileSize = 0
      this.recordingState.audioBuffer = []

      // å¼€å§‹è®¡æ—¶
      this.startRecordingTimer()

      // å¼€å§‹éŸ³é¢‘å¤„ç†
      this.startAudioProcessing()
      console.log('ï¿½ï¸ Audio recording started successfully')

      this.emit('recordingStarted')
    } catch (error) {
      console.error('Failed to start recording:', error)
      this.emit('permissionDenied', error)
      throw error
    }
  }

  stopRecording(): void {
    if (!this.recordingState.isRecording) {
      return
    }

    // åœæ­¢å½•åˆ¶
    this.recordingState.isRecording = false
    this.recordingState.isPaused = false

    // æ¸…ç†èµ„æº
    this.cleanupAudioProcessing()
    this.cleanupMediaStream()

    // åœæ­¢è®¡æ—¶
    this.stopRecordingTimer()

    this.emit('recordingStopped', {
      duration: this.recordingState.duration,
      bufferSize: this.recordingState.audioBuffer.length
    })
  }

  pauseRecording(): void {
    if (!this.recordingState.isRecording || this.recordingState.isPaused) {
      return
    }

    this.recordingState.isPaused = true
    this.stopRecordingTimer()
    this.emit('recordingPaused')
  }

  resumeRecording(): void {
    if (!this.recordingState.isRecording || !this.recordingState.isPaused) {
      return
    }

    this.recordingState.isPaused = false
    this.startRecordingTimer()
    this.emit('recordingResumed')
  }

  async playAudio(audioData: ArrayBuffer): Promise<void> {
    if (!this.audioContext || !this.audioElement) {
      throw new Error('Audio not initialized')
    }

    try {
      // åœæ­¢å½“å‰æ’­æ”¾
      this.stopPlayback()

      this.ensurePlaybackGainNode()

      // åˆ›å»ºBlob URL
      const blob = new Blob([audioData], { type: 'audio/wav' })
      const url = URL.createObjectURL(blob)
      this.currentPlaybackUrl = url

      // è®¾ç½®éŸ³é¢‘æº
      this.audioElement.src = url

      // è®¾ç½®éŸ³é‡
      this.audioElement.volume = this.config.outputVolume

      // æ’­æ”¾éŸ³é¢‘
      await this.audioElement.play()

      this.playbackState.isPlaying = true
      this.playbackState.isPaused = false
      this.playbackState.currentTime = 0

      // ç›‘å¬æ’­æ”¾ç»“æŸ
      this.audioElement.onended = () => {
        this.stopPlayback()
        URL.revokeObjectURL(url)
        this.currentPlaybackUrl = null
      }

      // æ›´æ–°æ’­æ”¾è¿›åº¦
      this.updatePlaybackProgress()

      this.emit('playbackStarted')
    } catch (error) {
      console.error('Failed to play audio:', error)
      throw error
    }
  }

  beginStreamingPlayback(): void {
    if (!this.audioContext) {
      throw new Error('Audio not initialized')
    }

    this.clearStreamingStopTimeout()
    this.stopStreamingSources()
    this.streamingSources = []

    this.streamingState.isActive = true
    this.streamingState.nextStartTime = this.audioContext.currentTime + 0.05
    this.streamingState.hasChunks = false

    if (this.audioElement) {
      this.audioElement.pause()
      this.audioElement.currentTime = 0
    }

    if (this.playbackState.isPlaying) {
      this.playbackState.isPlaying = false
      this.playbackState.isPaused = false
      this.playbackState.currentTime = 0
      this.playbackState.duration = 0
      this.emit('playbackStopped')
    }
  }

  hasStreamedAudio(): boolean {
    return this.streamingState.hasChunks
  }

  async enqueueAudioChunk(chunk: ArrayBuffer, sampleRate?: number): Promise<void> {
    if (!this.audioContext) {
      throw new Error('Audio not initialized')
    }

    if (!this.streamingState.isActive) {
      try {
        this.beginStreamingPlayback()
      } catch (error) {
        console.error('Failed to auto-start streaming playback:', error)
      }
    }

    this.ensurePlaybackGainNode()

    try {
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume().catch(() => undefined)
      }

      const extracted = this.extractPcmData(chunk, sampleRate)
      if (!extracted) {
        return
      }

      const targetRate = this.audioContext.sampleRate
      const samples = extracted.sampleRate && extracted.sampleRate !== targetRate
        ? this.resampleFloat32(extracted.samples, extracted.sampleRate, targetRate)
        : extracted.samples

      if (samples.length === 0) {
        return
      }

      const buffer = this.audioContext.createBuffer(1, samples.length, targetRate)
      buffer.copyToChannel(samples, 0)

      const source = this.audioContext.createBufferSource()
      source.buffer = buffer
      source.connect(this.playbackGainNode ?? this.audioContext.destination)

      const startTime = Math.max(this.streamingState.nextStartTime, this.audioContext.currentTime + 0.01)
      source.start(startTime)

      this.streamingState.nextStartTime = startTime + buffer.duration
      this.streamingState.hasChunks = true
      this.streamingSources.push(source)

      source.onended = () => {
        this.streamingSources = this.streamingSources.filter(active => active !== source)

        if (!this.streamingState.isActive && this.streamingSources.length === 0) {
          this.completeStreamingPlayback()
        }
      }

      if (!this.playbackState.isPlaying) {
        this.playbackState.isPlaying = true
        this.playbackState.isPaused = false
        this.emit('playbackStarted')
      }
    } catch (error) {
      console.error('Failed to enqueue audio chunk:', error)
    }
  }

  finishStreamingPlayback(): void {
    this.streamingState.isActive = false

    if (!this.audioContext) {
      this.completeStreamingPlayback()
      return
    }

    if (this.streamingSources.length === 0) {
      this.completeStreamingPlayback()
      return
    }

    const remainingMs = Math.max(100, (this.streamingState.nextStartTime - this.audioContext.currentTime) * 1000 + 100)
    this.clearStreamingStopTimeout()
    this.streamingStopTimeout = window.setTimeout(() => this.completeStreamingPlayback(), remainingMs)
  }

  private ensurePlaybackGainNode(): void {
    if (!this.audioContext) {
      return
    }

    if (!this.playbackGainNode) {
      this.playbackGainNode = this.audioContext.createGain()
      this.playbackGainNode.connect(this.audioContext.destination)
    }

    this.playbackGainNode.gain.value = this.config.outputVolume
  }

  private extractPcmData(buffer: ArrayBuffer, providedSampleRate?: number): { samples: Float32Array; sampleRate: number } | null {
    if (!buffer || buffer.byteLength < 2) {
      return null
    }

    const view = new DataView(buffer)
    const RIFF = 0x52494646
    const WAVE = 0x57415645
    const DATA = 0x64617461
    const FMT = 0x666d7420

    const isRiff = view.byteLength >= 12 && view.getUint32(0, false) === RIFF && view.getUint32(8, false) === WAVE

    if (isRiff) {
      let offset = 12
      let sampleRate = providedSampleRate ?? 24000
      let bitsPerSample = 16

      while (offset + 8 <= view.byteLength) {
        const chunkId = view.getUint32(offset, false)
        const chunkSize = view.getUint32(offset + 4, true)
        const dataOffset = offset + 8

        if (chunkId === FMT && chunkSize >= 16) {
          const fmtSampleRate = view.getUint32(dataOffset + 4, true)
          const fmtBitsPerSample = view.getUint16(dataOffset + 14, true)
          sampleRate = fmtSampleRate || sampleRate
          bitsPerSample = fmtBitsPerSample || bitsPerSample
        }

        if (chunkId === DATA) {
          const slice = buffer.slice(dataOffset, dataOffset + chunkSize)
          if (bitsPerSample !== 16) {
            console.warn('Unsupported bitsPerSample for streaming audio chunk:', bitsPerSample)
            return null
          }
          const pcm = new Int16Array(slice)
          if (pcm.length === 0) {
            return null
          }
          return {
            samples: this.pcm16ToFloat32(pcm),
            sampleRate
          }
        }

        offset += 8 + chunkSize
      }
    }

    const pcmSamples = new Int16Array(buffer)
    if (pcmSamples.length === 0) {
      return null
    }

    return {
      samples: this.pcm16ToFloat32(pcmSamples),
      sampleRate: providedSampleRate ?? 24000
    }
  }

  private pcm16ToFloat32(samples: Int16Array): Float32Array {
    const float32 = new Float32Array(samples.length)
    const divisor = 32768
    for (let i = 0; i < samples.length; i++) {
      const value = samples[i] / divisor
      float32[i] = Math.max(-1, Math.min(1, value))
    }
    return float32
  }

  private resampleFloat32(input: Float32Array, inputRate: number, targetRate: number): Float32Array {
    if (inputRate === targetRate || input.length === 0) {
      return input
    }

    const ratio = inputRate / targetRate
    if (ratio <= 0) {
      return input
    }

    const outputLength = Math.max(1, Math.round(input.length / ratio))
    const output = new Float32Array(outputLength)

    let position = 0
    for (let i = 0; i < outputLength; i++) {
      const index = Math.floor(position)
      const nextIndex = Math.min(index + 1, input.length - 1)
      const weight = position - index
      output[i] = input[index] * (1 - weight) + input[nextIndex] * weight
      position += ratio
    }

    return output
  }

  private completeStreamingPlayback(): void {
    this.clearStreamingStopTimeout()

    this.stopStreamingSources()
    this.streamingSources = []
    this.streamingState.nextStartTime = 0
    this.streamingState.isActive = false

    if (this.playbackState.isPlaying) {
      this.playbackState.isPlaying = false
      this.playbackState.isPaused = false
      this.playbackState.currentTime = 0
      this.playbackState.duration = 0
      this.emit('playbackStopped')
    }
  }

  private stopStreamingSources(): void {
    if (this.streamingSources.length === 0) {
      return
    }

    for (const source of this.streamingSources) {
      try {
        source.onended = null
        source.stop()
      } catch (error) {
        console.warn('Failed to stop streaming source:', error)
      }
    }
  }

  private clearStreamingStopTimeout(): void {
    if (this.streamingStopTimeout) {
      window.clearTimeout(this.streamingStopTimeout)
      this.streamingStopTimeout = null
    }
  }

  stopPlayback(): void {
    this.clearStreamingStopTimeout()
    this.stopStreamingSources()
    this.streamingSources = []
    this.streamingState.isActive = false
    this.streamingState.nextStartTime = 0
    this.streamingState.hasChunks = false

    if (this.audioElement) {
      this.audioElement.onended = null
      this.audioElement.pause()
      this.audioElement.currentTime = 0
      this.audioElement.src = ''
    }

    if (this.currentPlaybackUrl) {
      URL.revokeObjectURL(this.currentPlaybackUrl)
      this.currentPlaybackUrl = null
    }

    if (this.playbackState.isPlaying) {
      this.playbackState.isPlaying = false
      this.playbackState.isPaused = false
      this.playbackState.currentTime = 0
      this.playbackState.duration = 0
      this.emit('playbackStopped')
    }
  }

  pausePlayback(): void {
    if (!this.playbackState.isPlaying || this.playbackState.isPaused) {
      return
    }

    if (this.audioElement) {
      this.audioElement.pause()
    }

    this.playbackState.isPaused = true
    this.emit('playbackPaused')
  }

  resumePlayback(): void {
    if (!this.playbackState.isPlaying || !this.playbackState.isPaused) {
      return
    }

    if (this.audioElement) {
      this.audioElement.play()
    }

    this.playbackState.isPaused = false
    this.emit('playbackResumed')
  }

  setVolume(level: number): void {
    this.config.outputVolume = Math.max(0, Math.min(1, level))

    if (this.audioElement) {
      this.audioElement.volume = this.config.outputVolume
    }

    if (this.playbackGainNode) {
      this.playbackGainNode.gain.value = this.config.outputVolume
    }

    this.emit('volumeChanged', this.config.outputVolume)
  }

  getVolumeLevel(): number {
    if (!this.analyserNode) {
      return 0
    }

    const timeData = new Float32Array(this.analyserNode.fftSize)
    this.analyserNode.getFloatTimeDomainData(timeData)

    let sumOfSquares = 0
    let peak = 0
    for (let i = 0; i < timeData.length; i++) {
      const sample = timeData[i]
      sumOfSquares += sample * sample
      peak = Math.max(peak, Math.abs(sample))
    }
    const rms = Math.sqrt(sumOfSquares / timeData.length)
    let magnitude = Math.max(rms, peak)

    if (magnitude < 0.01) {
      const freqData = new Uint8Array(this.analyserNode.frequencyBinCount)
      this.analyserNode.getByteFrequencyData(freqData)
      let sum = 0
      for (let i = 0; i < freqData.length; i++) {
        sum += freqData[i]
      }
      const avgFrequency = (sum / freqData.length) / 255
      magnitude = Math.max(magnitude, avgFrequency)
    }

    let volume = Math.min(100, Math.max(0, Math.sqrt(magnitude) * 100))

    const smoothFactor = 0.2 // ä½¿éŸ³é‡å˜åŒ–æ›´å¹³æ»‘
    volume = this.lastVolume * (1 - smoothFactor) + volume * smoothFactor
    this.lastVolume = volume

    const volumePercent = Math.round(volume)
    
    if (volumePercent > 0) {
      console.log('ğŸ”Š Volume detected:', {
        rawRms: rms.toFixed(4),
        peak: peak.toFixed(4),
        final: volumePercent
      })
    }
    
    return volumePercent
  }

  getVisualizationData(): AudioVisualizationData {
    if (!this.analyserNode) {
      return {
        volume: 0,
        frequency: new Uint8Array(0),
        timeData: new Uint8Array(0),
        timestamp: Date.now()
      }
    }

    const frequencyData = new Uint8Array(this.analyserNode.frequencyBinCount)
    const timeData = new Uint8Array(this.analyserNode.fftSize)

    this.analyserNode.getByteFrequencyData(frequencyData)
    this.analyserNode.getByteTimeDomainData(timeData)

    return {
      volume: this.getVolumeLevel(), // ç°åœ¨è¿”å›0-100èŒƒå›´
      frequency: frequencyData,
      timeData,
      timestamp: Date.now()
    }
  }

  getRecordingState(): AudioRecordingState {
    return { ...this.recordingState }
  }

  getPlaybackState(): AudioPlaybackState {
    return { ...this.playbackState }
  }

  clearAudioBuffer(): void {
    this.recordingState.audioBuffer = []
    this.recordingState.currentBuffer = null
    this.recordingState.duration = 0
    this.recordingState.fileSize = 0
    this.emit('audioBufferCleared')
  }

  async getDevices(): Promise<AudioDevice[]> {
    try {
      // å¼ºåˆ¶æ›´æ–°è®¾å¤‡åˆ—è¡¨
      await this.updateDeviceList()
      return [...this.devices]
    } catch (error) {
      console.error('Failed to get devices:', error)
      return []
    }
  }

  setConnectionReady(ready: boolean): void {
    this.connectionReady = ready
  }

  isVoiceActive(): boolean {
    return this.vadDetector?.isVoiceActive() || false;
  }

  getVadVolumeLevel(): number {
    return this.vadDetector?.getVolume() ?? 0;
  }

  getAudioBuffer(): AudioBuffer | null {
    return this.recordingState.currentBuffer;
  }

  updateVadConfig(config: Partial<{ threshold: number; silenceDuration: number }>): void {
    const normalized = { ...config }
    if (typeof normalized.threshold === 'number' && normalized.threshold <= 1) {
      normalized.threshold = normalized.threshold * 100
    }
    this.vadDetector?.updateConfig(normalized)
  }

  // è·å–éŸ³é¢‘ä¸Šä¸‹æ–‡ï¼ˆä¾›VADä½¿ç”¨ï¼‰
  getAudioContext(): AudioContext | null {
    return this.audioContext;
  }

  // è·å–åˆ†æå™¨èŠ‚ç‚¹ï¼ˆä¾›VADä½¿ç”¨ï¼‰
  getAnalyserNode(): AnalyserNode | null {
    return this.analyserNode;
  }

  private setupAudioProcessing(): void {
    if (!this.audioContext || !this.mediaStream) {
      return
    }

    // åˆ›å»ºæºèŠ‚ç‚¹
    this.sourceNode = this.audioContext.createMediaStreamSource(this.mediaStream)

    // åˆ›å»ºå¢ç›ŠèŠ‚ç‚¹ (ç”¨äºæé«˜è¾“å…¥éŸ³é‡æ•æ„Ÿåº¦)
    this.gainNode = this.audioContext.createGain()
    this.gainNode.gain.value = 20.0 // å¢åŠ å¢ç›Šä»¥æé«˜æ•æ„Ÿåº¦

    // åˆ›å»ºåˆ†æå™¨èŠ‚ç‚¹ (ç”¨äºéŸ³é‡å¯è§†åŒ–å’ŒVAD)
    this.analyserNode = this.audioContext.createAnalyser()
    this.analyserNode.fftSize = 2048
    
    // è¿æ¥èŠ‚ç‚¹ï¼šæº -> å¢ç›Š -> åˆ†æå™¨ (ç”¨äºéŸ³é‡æ£€æµ‹)
    this.sourceNode.connect(this.gainNode)
    this.gainNode.connect(this.analyserNode)
    // å°†åˆ†æå™¨è¾“å‡ºè¿æ¥åˆ°é™éŸ³å¢ç›ŠèŠ‚ç‚¹ï¼Œé˜²æ­¢éŸ³é¢‘è¢«å¬åˆ°çš„åŒæ—¶ä¿æŒå¤„ç†é“¾è·¯æ¿€æ´»
    this.monitorGainNode = this.audioContext.createGain()
    this.monitorGainNode.gain.value = 0
    this.analyserNode.connect(this.monitorGainNode)
    this.monitorGainNode.connect(this.audioContext.destination)

    // åˆå§‹åŒ–VADæ£€æµ‹å™¨ (éœ€è¦åœ¨åˆ†æå™¨è®¾ç½®å®Œæˆå)
    if (this.vadDetector && this.audioContext && this.analyserNode) {
      this.vadDetector.initialize(this.audioContext, this.analyserNode);
    }
  }

  private startAudioProcessing(): void {
    if (!this.analyserNode) {
      console.log('ğŸ¤ No analyser node available')
      return
    }

    let audioFrameCount = 0
    let lastAudioSendTime = 0
    let lastVisualizationTime = 0
    const audioSendInterval = 50 // æ¯50mså‘é€ä¸€æ¬¡éŸ³é¢‘æ•°æ®
    const visualizationInterval = 16 // æ¯16msæ›´æ–°ä¸€æ¬¡å¯è§†åŒ–æ•°æ®ï¼ˆ60fpsï¼‰

    const processAudio = () => {
      if (this.recordingState.isRecording && !this.recordingState.isPaused) {
        audioFrameCount++
        const currentTime = Date.now()

        // è·å–éŸ³é¢‘æ•°æ®
        const bufferLength = this.analyserNode?.fftSize || 2048
        const dataArray = new Float32Array(bufferLength)
        this.analyserNode?.getFloatTimeDomainData(dataArray)

        // è½¬æ¢ä¸ºPCM16æ ¼å¼
        const pcmData = this.convertToPCM16(dataArray)

        // åˆ›å»ºéŸ³é¢‘ç¼“å†²åŒº
        const dataBuffer = pcmData.buffer.slice(0)
        const audioBuffer: AudioBuffer = {
          data: dataBuffer,
          format: 'pcm16',
          sampleRate: this.config.sampleRate,
          duration: pcmData.length / this.config.sampleRate,
          timestamp: currentTime
        }

        // æ·»åŠ åˆ°ç¼“å†²åŒº
        this.recordingState.audioBuffer.push(audioBuffer)
        this.recordingState.fileSize += pcmData.buffer.byteLength

        // æ›´æ–°å½“å‰ç¼“å†²åŒº
        this.recordingState.currentBuffer = audioBuffer

        // æ§åˆ¶éŸ³é¢‘æ•°æ®å‘é€é¢‘ç‡
        if (currentTime - lastAudioSendTime >= audioSendInterval) {
          lastAudioSendTime = currentTime

          // æŒç»­å‘é€æ‰€æœ‰éŸ³é¢‘æ•°æ®ï¼Œè®©æœåŠ¡å™¨å¤„ç†VAD
          const audioData = {
            buffer: audioBuffer,
            base64: this.arrayBufferToBase64(pcmData.buffer)
          }

          // è°ƒè¯•ï¼šæ¯20æ¬¡å‘é€æ‰“å°ä¸€æ¬¡éŸ³é¢‘æ•°æ®ä¿¡æ¯
          if (audioFrameCount % 20 === 0) {
            console.log('ğŸµ Audio frame sent:', {
              frame: audioFrameCount,
              bufferSize: pcmData.buffer.byteLength,
              sampleRate: this.config.sampleRate,
              base64Length: audioData.base64.length
            })
          }

          this.emit('audioData', audioData)
        }

        // å‘é€å¯è§†åŒ–æ•°æ®äº‹ä»¶ï¼ˆæ›´é¢‘ç¹ï¼‰
        if (currentTime - lastVisualizationTime >= visualizationInterval) {
          lastVisualizationTime = currentTime
          const vizData = this.getVisualizationData()
          this.emit('visualizationData', vizData)
          
          // è°ƒè¯•ï¼šæ¯30æ¬¡æ‰“å°ä¸€æ¬¡éŸ³é‡ä¿¡æ¯
          if (audioFrameCount % 30 === 0) {
            console.log('ğŸ”Š Visualization data:', {
              volume: vizData.volume,
              timestamp: vizData.timestamp
            })
          }
        }
      } else {
        // å¦‚æœä¸åœ¨å½•éŸ³çŠ¶æ€ï¼Œåœæ­¢éŸ³é¢‘å¤„ç†å¾ªç¯
        if (this.animationFrame) {
          cancelAnimationFrame(this.animationFrame)
          this.animationFrame = null
        }
      }

      if (this.recordingState.isRecording) {
        this.animationFrame = requestAnimationFrame(processAudio)
      }
    }

    this.animationFrame = requestAnimationFrame(processAudio)
    
    // å¯åŠ¨VADæ£€æµ‹
    if (this.vadDetector) {
      this.vadDetector.startDetection();
    }
  }

  private testMicrophoneInput(): void {
    // å¢åŠ å»¶è¿Ÿç­‰å¾…éŸ³é¢‘æµç¨³å®š
    setTimeout(() => {
      if (!this.analyserNode || !this.mediaStream) {
        return
      }

      // æ£€æŸ¥è½¨é“çŠ¶æ€
      const tracks = this.mediaStream.getTracks()
    }, 2000) // å¢åŠ åˆ°2ç§’å»¶è¿Ÿ
  }

  private convertToPCM16(float32Array: Float32Array): Int16Array {
    const pcm16Array = new Int16Array(float32Array.length)

    for (let i = 0; i < float32Array.length; i++) {
      // å°†æµ®ç‚¹æ•°è½¬æ¢ä¸º16ä½æ•´æ•°
      const sample = Math.max(-1, Math.min(1, float32Array[i]))
      pcm16Array[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF
    }

    return pcm16Array
  }

  arrayBufferToBase64(buffer: ArrayBuffer): string {
    let binary = ''
    const bytes = new Uint8Array(buffer)
    const len = bytes.byteLength

    for (let i = 0; i < len; i++) {
      binary += String.fromCharCode(bytes[i])
    }

    return btoa(binary)
  }

  private startRecordingTimer(): void {
    this.recordingTimer = window.setInterval(() => {
      if (this.recordingState.isRecording && !this.recordingState.isPaused) {
        this.recordingState.duration += 1
      }
    }, 1000)
  }

  private stopRecordingTimer(): void {
    if (this.recordingTimer) {
      clearInterval(this.recordingTimer)
      this.recordingTimer = null
    }
  }

  private updatePlaybackProgress(): void {
    if (!this.audioElement || !this.playbackState.isPlaying) {
      return
    }

    this.playbackState.currentTime = this.audioElement.currentTime
    this.playbackState.duration = this.audioElement.duration || 0

    if (this.playbackState.isPlaying) {
      requestAnimationFrame(() => this.updatePlaybackProgress())
    }
  }

  private cleanupAudioProcessing(): void {
    if (this.animationFrame) {
      cancelAnimationFrame(this.animationFrame)
      this.animationFrame = null
    }

    // åœæ­¢VADæ£€æµ‹
    if (this.vadDetector) {
      this.vadDetector.stopDetection();
    }

    if (this.sourceNode) {
      this.sourceNode.disconnect()
      this.sourceNode = null
    }

    if (this.analyserNode) {
      this.analyserNode.disconnect()
      this.analyserNode = null
    }

    if (this.gainNode) {
      this.gainNode.disconnect()
      this.gainNode = null
    }

    if (this.monitorGainNode) {
      this.monitorGainNode.disconnect()
      this.monitorGainNode = null
    }
  }

  private cleanupMediaStream(): void {
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop())
      this.mediaStream = null
    }
  }

  private async updateDeviceList(): Promise<void> {
    try {
      const devices = await navigator.mediaDevices.enumerateDevices()
      
      this.devices = devices
        .filter(device => device.kind === 'audioinput' || device.kind === 'audiooutput')
        .map(device => ({
          deviceId: device.deviceId,
          label: device.label || `${device.kind === 'audioinput' ? 'Microphone' : 'Speaker'} ${device.deviceId.slice(0, 8)}`,
          kind: device.kind as 'audioinput' | 'audiooutput',
          groupId: device.groupId,
          isDefault: device.deviceId === 'default' || device.label.toLowerCase().includes('default')
        }))
      
    } catch (error) {
      console.error('Failed to update device list:', error)
      this.devices = []
    }
  }

  async validateDevice(deviceId: string): Promise<boolean> {
    try {
      const constraints = {
        audio: {
          deviceId: { exact: deviceId },
          sampleRate: this.config.sampleRate,
          channelCount: this.config.channelCount,
          echoCancellation: this.config.echoCancellation,
          noiseSuppression: this.config.noiseSuppression,
          autoGainControl: this.config.autoGainControl
        }
      }

      const stream = await navigator.mediaDevices.getUserMedia(constraints)
      const track = stream.getAudioTracks()[0]

      // åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„AudioContextæ¥æµ‹è¯•éŸ³é¢‘è¾“å…¥
      const tempContext = new AudioContext({ sampleRate: this.config.sampleRate })
      const source = tempContext.createMediaStreamSource(stream)
      const analyser = tempContext.createAnalyser()
      analyser.fftSize = 2048
      source.connect(analyser)

      // æ£€æµ‹æ˜¯å¦æœ‰éŸ³é¢‘æ´»åŠ¨
      const dataArray = new Uint8Array(analyser.frequencyBinCount)
      let hasAudio = false

      // æ£€æµ‹3ç§’å†…çš„éŸ³é¢‘æ´»åŠ¨
      for (let i = 0; i < 30; i++) { // æ¯100msæ£€æŸ¥ä¸€æ¬¡
        await new Promise(resolve => setTimeout(resolve, 100))
        analyser.getByteFrequencyData(dataArray)
        
        // æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°éŸ³é¢‘æ´»åŠ¨
        for (let j = 0; j < dataArray.length; j++) {
          if (dataArray[j] > 10) { // æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„éŸ³é¢‘æ´»åŠ¨
            hasAudio = true
            break
          }
        }
        
        if (hasAudio) break
      }

      // æ¸…ç†èµ„æº
      stream.getTracks().forEach(track => track.stop())
      await tempContext.close()

      return hasAudio
    } catch (error) {
      console.error(`Error validating device ${deviceId}:`, error)
      return false
    }
  }

  async findBestAudioInputDevice(): Promise<string | null> {
    try {
      // é¦–å…ˆè·å–æœ€æ–°çš„è®¾å¤‡åˆ—è¡¨
      await this.updateDeviceList()
      
      // è·å–æ‰€æœ‰éŸ³é¢‘è¾“å…¥è®¾å¤‡
      const audioInputs = this.devices.filter(device => device.kind === 'audioinput')
      
      if (audioInputs.length === 0) {
        console.log('No audio input devices found')
        return null
      }

      // é¦–å…ˆæ£€æŸ¥é»˜è®¤è®¾å¤‡æ˜¯å¦æœ‰å£°éŸ³
      const defaultDevice = audioInputs.find(device => device.isDefault)
      if (defaultDevice) {
        console.log('Testing default device:', defaultDevice.label)
        const isValid = await this.validateDevice(defaultDevice.deviceId)
        if (isValid) {
          console.log('Default device is working:', defaultDevice.label)
          return defaultDevice.deviceId
        } else {
          console.log('Default device is not working:', defaultDevice.label)
        }
      }

      // å¦‚æœé»˜è®¤è®¾å¤‡ä¸å¯ç”¨ï¼Œæµ‹è¯•å…¶ä»–è®¾å¤‡
      for (const device of audioInputs) {
        if (device.isDefault) continue // å·²æµ‹è¯•é»˜è®¤è®¾å¤‡
        
        console.log('Testing device:', device.label)
        const isValid = await this.validateDevice(device.deviceId)
        if (isValid) {
          console.log('Found working device:', device.label)
          return device.deviceId
        } else {
          console.log('Device is not working:', device.label)
        }
      }

      // å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„è®¾å¤‡ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè®¾å¤‡
      console.log('No working devices found, using first available device')
      return audioInputs[0].deviceId
    } catch (error) {
      console.error('Error finding best audio input device:', error)
      return null
    }
  }

  dispose(): void {
    this.stopRecording()
    this.stopPlayback()
    this.cleanupAudioProcessing()
    this.cleanupMediaStream()
    this.connectionReady = false

    if (this.audioContext) {
      this.audioContext.close()
      this.audioContext = null
    }

    if (this.audioElement) {
      this.audioElement = null
    }

    // æ¸…ç†VADæ£€æµ‹å™¨
    if (this.vadDetector) {
      this.vadDetector.stopDetection();
      this.vadDetector = null;
    }

    this.removeAllListeners()
  }
}

export default AudioManager
